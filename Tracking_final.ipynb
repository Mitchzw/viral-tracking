{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c25665",
   "metadata": {},
   "source": [
    "### Track analysis of viral particles (influenza), incubated on CHO cells expressing human transferrin receptor\n",
    "\n",
    "collaboration with Beryl Mazel-Sanchez from Mirko Schmolke's lab\n",
    "\n",
    "V03, 06/07/2022, Michael Bachmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9275b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation: \n",
    "\n",
    "# files from branch tracing (= summary of tracks) and\n",
    "# files \"tracks\" (= details about tracks including intensity in \n",
    "# different channels) \n",
    "\n",
    "# files named with date of experiment in format \"yyyymmdd\" and split from other information in file name\n",
    "# by \"_\"\n",
    "\n",
    "# identifier of analyzed cell in the file name is a number with maximally 2 digits, separated\n",
    "# from anything else by a space, and there is no other number in the file name with one or two digits\n",
    "# (relies on automatic annotation of ImageJ for different scenes of a video; \"series 1\", \"series 2\", etc)\n",
    "\n",
    "# Videos have to be 30 min long (31 time points) with one frame every min\n",
    "\n",
    "# here, data is assumed to be based on pixel scaling, not micron. values will be transformed into micron values \n",
    "# based on 1 micron being 7.8604 px / 1 px = 0.1272 px\n",
    "\n",
    "# general directory that contains subfolder named \"all\" that contains all files\n",
    "directory = '/Users/michaelbachmann/Python/viral particles/final analysis/BFP_HA VLP/'\n",
    "\n",
    "path_all = directory + 'all/'\n",
    "\n",
    "# factor for bleach correction, calculated separately\n",
    "bleach_cor = 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95112b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bunch of functions that are called later\n",
    "\n",
    "def ident_replicate(file_name):\n",
    "    \"\"\"\n",
    "    file name will be split based on '_' \n",
    "    \n",
    "    --> Make sure that filename has date \n",
    "    separated by underscore and has date in format yyyymmdd. Date is recognized \n",
    "    as being an integer > 20000000 meaning that having another integer in the \n",
    "    file name that is not the date of experiment will create problems.\n",
    "    \n",
    "    Input is name of a file, return value is date of experiment as integer \n",
    "    \"\"\"\n",
    "    \n",
    "    file_parts = file_name.split('_') #split file name in parts identified by \"_\" between them\n",
    "    \n",
    "    exp_date = 0 #variable to hold date of experiment\n",
    "    \n",
    "    # this loop goes through the split file name and looks with \"try int()\" for possible \n",
    "    # integers; if an integer is bigger than 2000 00 00 it is considered to be a date \n",
    "    # and is used as output of the function\n",
    "    for index, i in enumerate(file_parts):\n",
    "        try:\n",
    "            date = int(file_parts[index])\n",
    "            if date > 20000000:\n",
    "                exp_date = date\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return(exp_date)\n",
    "\n",
    "def ident_cell (file_name):\n",
    "    \"\"\"\n",
    "    Identify cell from \"series ...\" information in file name. Identifier needs to be an integer\n",
    "    that is separated from other information with spaces and that has maximally two digits.\n",
    "    \n",
    "    Input is name of a file, return is the cell number as writen in the file name.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_parts = file_name.split(' ') #split file name in parts identified by \"_\" between them\n",
    "    \n",
    "    cell_number = -1\n",
    "    \n",
    "    for index, i in enumerate(file_parts):\n",
    "        if len(file_parts[index]) < 3:\n",
    "            \n",
    "            try:\n",
    "                cell_number = int(file_parts[index])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        else:\n",
    "            if cell_number == -1: \n",
    "                cell_number = 'NA' #if cell_number hasn't been changed (because no cell nr could have been \n",
    "                                   #found) cell_number will be returned as 'NA' to indicate a problem\n",
    "\n",
    "    return(cell_number)\n",
    "\n",
    "def is_file_empty_3(file_name):\n",
    "    \"\"\" Check if file is empty by reading first character in it. Return will be True if file is empty or\n",
    "    false when file contains something already.\n",
    "    \"\"\"\n",
    "    # open file in read mode\n",
    "    \n",
    "    with open(file_name, 'r') as read_obj:\n",
    "        # read first character\n",
    "        one_char = read_obj.read(1)\n",
    "        # if not fetched then file is empty\n",
    "        if not one_char:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def create_sum (file_sum_path, file_sum_name, counter, replicate_id, cell_tracker, min_distance=0, min_veloc=0):\n",
    "    \"\"\" \n",
    "    Read data from file and create new file with reduced amount of information.\n",
    "    \n",
    "    Input is path of file (file_sum_path), file name (file_sum_name), counter was initially used to identify\n",
    "    cells but is currently not used anymore, cell_tracker obtained from function 'ident_cell' to identify \n",
    "    separate cells, and date of experiment as ID for independent experiments (replicate_id)\n",
    "    \n",
    "    Tracks below a certain distance length or velocity can be discarded with optional values\n",
    "    min_distance and min_veloc. Otherwise these values are set to 0. Condition for both is '>=' compared\n",
    "    to chosen value\n",
    "    \"\"\"\n",
    "    \n",
    "    #output file will be named as input file with \"output\" added and format will be tsv instead of csv\n",
    "    file_sum_out = file_sum_name[0:-4] + '_output.tsv'\n",
    "    \n",
    "    with open (file_sum_path, 'r') as f_in, open (file_sum_out, 'w') as f_out:\n",
    "        \n",
    "        # index for counting lines\n",
    "        i = 1\n",
    "        \n",
    "        #printing header for output file\n",
    "        print ('replicate', 'cell', 'ID', 'track duration (min)', \n",
    "               'track distance (micron)', 'track velocity (micron/min)', \n",
    "               sep='\\t', file=f_out)\n",
    "        \n",
    "        #skipping header in in put file\n",
    "        while i < 5:\n",
    "            header = f_in.readline()\n",
    "            i += 1\n",
    "        \n",
    "        #read through lines and gather needed data for output file\n",
    "        for line in f_in:\n",
    "            data = line.strip().split(',')\n",
    "            track_id = int(data [1])\n",
    "            track_duration = float(data [4])\n",
    "            track_distance = float(data [5])\n",
    "            track_veloc = float(data [6])\n",
    "            \n",
    "            if (track_distance >= min_distance) and (track_veloc >= min_veloc) :\n",
    "                print (replicate_id, cell_tracker, track_id, track_duration, (track_distance/7.8604),\n",
    "                      (track_veloc/7.8604), sep='\\t', file=f_out)\n",
    "                #division of distance and velocity by 7.8604 because of scaling factor between px and micron\n",
    "            \n",
    "def create_detail (file_detail_path, file_detail_name, counter, replicate_id, \n",
    "                   bleach_correction, cell_tracker, track_length=1):\n",
    "    \"\"\" \n",
    "    Read tracking data from TrackMate file. Problem: data isn't sorted according to time frame for given \n",
    "    track \n",
    "    \n",
    "    This function reads through the file and creates a new dictionary for every track with the whole line \n",
    "    as value and the time frame as key. This dic will be sorted according to their key values = time frame\n",
    "    \n",
    "    After this sorting, the dictionary will be read key by key and the whole line will be split and\n",
    "    the necessary information is saved in a new file. After this, the dic will be deleted to be ready\n",
    "    for the data from the next track ID.\n",
    "    \n",
    "    Inputs are file path (file_detail_path), file name (file_detail_name), counter to identify separate cells \n",
    "    (not used anymore), date of experiment as ID for independent experiments (replicate_id), \n",
    "    a value for bleach correction that was identified independently (bleach_correction), a cell_tracker nr \n",
    "    obtained with function ident_cell, and an optional value for the minimum length of a track \n",
    "    (track length) with default value of 1. Condition for track length is >=.\n",
    "    \"\"\"\n",
    "    \n",
    "    # this part skips the header and stores the maximal number of measurements in line_max\n",
    "    # line_max is needed to identify and safe the last entry in the actual run\n",
    "    with open (file_detail_path, 'r') as f_in:\n",
    "\n",
    "        line_max = 0\n",
    "        i = 1\n",
    "\n",
    "        while i < 5:            #skip header\n",
    "            f_in.readline()\n",
    "            i += 1\n",
    "\n",
    "        for line in f_in:      #count number of lines with data\n",
    "            line_max += 1\n",
    "    \n",
    "        \n",
    "    # actual program to read and sort entries per track ID\n",
    "    \n",
    "    # create file names for the general file and the one with transposed values for one feature\n",
    "    # an additional file in append mode is created to list all tracks according to \"no entry\" (= track \n",
    "    # still present in time frame 30) or \"entry\" (not present in time frame 30)\n",
    "    f_d_o_e = file_detail_name[0:-4] + '_output_entry_bleach-cor.tsv'\n",
    "    f_d_o_ne = file_detail_name[0:-4] + '_output_NOentry_bleach-cor.tsv'\n",
    "    f_d_o_e_t = file_detail_name[0:-4] + '_output_entry_trans_median2.tsv'\n",
    "    f_d_o_ne_t = file_detail_name[0:-4] + '_output_NOentry_trans_median2.tsv'\n",
    "    e_vs_ne_output = 'entry_vs_no-entry_file.tsv'\n",
    "    \n",
    "    with open (file_detail_path, 'r') as f_in, open (f_d_o_e, 'w') as f_out_e, open (f_d_o_ne, 'w') as f_out_ne, open (f_d_o_e_t, 'w') as f_out_e_t, open (f_d_o_ne_t, 'w') as f_out_ne_t, open (e_vs_ne_output, 'a') as f_out_entry:\n",
    "        \n",
    "        i = 1\n",
    "        \n",
    "        # print header in new file with columns we need\n",
    "        print ('replicate', 'cell', 'ID', 'frame', 'median int ch 1', 'median int ch 1 corrected',\n",
    "               'median int ch 2', 'median int ch 2 corrected', 'median int ch 3', 'median int ch 3 corrected',\n",
    "               'contrast ch 1', 'contrast ch 2', 'contrast ch 3',\n",
    "               sep='\\t', file=f_out_e) \n",
    "        print ('replicate', 'cell', 'ID', 'frame', 'median int ch 1', 'median int ch 1 corrected',\n",
    "               'median int ch 2', 'median int ch 2 corrected', 'median int ch 3', 'median int ch 3 corrected',\n",
    "               'contrast ch 1', 'contrast ch 2', 'contrast ch 3',\n",
    "               sep='\\t', file=f_out_ne)\n",
    "        print ('ID', 'cell', 'track', 'median int 2 ch 2 corrected', sep='\\t', file=f_out_e_t)\n",
    "        print ('ID', 'cell', 'track', 'median int 2 ch 2 corrected', sep='\\t', file=f_out_ne_t)\n",
    "        \n",
    "        # for entry/no entry file which is append mode: write header only in empty file. function from above will\n",
    "        # be called\n",
    "        if is_file_empty_3(e_vs_ne_output):\n",
    "            print ('replicate', 'cell', 'ID', 'entry vs no entry', sep='\\t', file=f_out_entry)\n",
    "        \n",
    "        #skip header in input file\n",
    "        while i < 5:\n",
    "            f_in.readline()\n",
    "            i += 1\n",
    "        \n",
    "        #id_identifier is later used to compare ID of new line with ID of line before to identify change \n",
    "        #to a new track\n",
    "        id_identifier = 0\n",
    "        \n",
    "        #dictionary that will hold all lines from the same track ID and that will be sorted according to\n",
    "        #frame value\n",
    "        dic_tracks = {}\n",
    "        \n",
    "        #variables for identifying lines and sorting and counting line nr to know when last line is reached\n",
    "        tr_id_global = 0\n",
    "        tr_frame_global = 0\n",
    "        line_counter = 0\n",
    "        \n",
    "        #read through lines\n",
    "        for line in f_in:\n",
    "            line_counter += 1\n",
    "            data = line.strip().split(',')\n",
    "            \n",
    "            #variables to measure ID and frame of the current line\n",
    "            tr_id_global = int(data [2])\n",
    "            tr_frame_global = int(data[8])\n",
    "            \n",
    "            #start a dictionary as long as track ID is the same AND loop hasn't reached \n",
    "            #the last entry\n",
    "            if (tr_id_global == id_identifier) and (line_counter != line_max):\n",
    "        \n",
    "                #create a dic where frame is used as key, whole line is value\n",
    "                #this allows later to sort all lines according to the frame \n",
    "                if not tr_frame_global in dic_tracks:\n",
    "                    dic_tracks[tr_frame_global] = []\n",
    "                \n",
    "                dic_tracks[tr_frame_global].append(line)\n",
    "            \n",
    "            # when loop reaches a line with a new, different track ID, the existing dic with \n",
    "            # all entries for the old track ID will be sorted according to their frame value\n",
    "            # afterwards, values of new track ID will be stored in file\n",
    "            elif (tr_id_global != id_identifier) and (line_counter != line_max):\n",
    "                \n",
    "                #actual sorting according to keys which is the number of frame in this dic\n",
    "                dic_tracks = dict(sorted(dic_tracks.items()))\n",
    "                \n",
    "                #test if track has at least minimal length \n",
    "                #track_length = 1 if no value has been used as input\n",
    "                if len(dic_tracks) >= track_length:\n",
    "                    \n",
    "                    #decide if track counts as entering track or not based on existence of time frame 30\n",
    "                    #functions will be called to write new file with sorted lines and another function\n",
    "                    #to create a transposed file of one measured value and additional information will\n",
    "                    #be saved into entry/no entry file for comparison later on\n",
    "                    if 30 in dic_tracks:\n",
    "                        sorting_lines(f_out_ne, dic_tracks, replicate_id, cell_tracker, bleach_correction)\n",
    "                        transposed_lines(f_out_ne_t, dic_tracks, replicate_id, cell_tracker, bleach_correction)\n",
    "                        print(replicate_id, cell_tracker, id_identifier, 'no entry', sep='\\t', file=f_out_entry)\n",
    "\n",
    "                    elif 30 not in dic_tracks:\n",
    "                        sorting_lines(f_out_e, dic_tracks, replicate_id, cell_tracker, bleach_correction)\n",
    "                        transposed_lines(f_out_e_t, dic_tracks, replicate_id, cell_tracker, bleach_correction)\n",
    "                        print(replicate_id, cell_tracker, id_identifier, 'entry', sep='\\t', file=f_out_entry)\n",
    "                    \n",
    "                    #for trouble shooting\n",
    "                    else:\n",
    "                        print('neither longer nor shorter than 30 tracks?')\n",
    "                \n",
    "                #now, dictionary will be emptied...\n",
    "                dic_tracks = {}\n",
    "                #...a new list will be created with the current frame as key...\n",
    "                dic_tracks[tr_frame_global] = []\n",
    "                #...and the current line will be saved linked to this key (=frame)\n",
    "                dic_tracks[tr_frame_global].append(line)\n",
    "                #updating id identifier to indicate the new track\n",
    "                id_identifier = tr_id_global\n",
    "            \n",
    "            # this will be triggered when the last line is reached in order to sort and save data from the \n",
    "            # last track\n",
    "            elif line_counter == line_max:\n",
    "                \n",
    "                #data from the current line has to be saved\n",
    "                if not tr_frame_global in dic_tracks:\n",
    "                    dic_tracks[tr_frame_global] = []\n",
    "                \n",
    "                dic_tracks[tr_frame_global].append(line)\n",
    "                \n",
    "                #do the sorting for the current track ID\n",
    "                dic_tracks = dict(sorted(dic_tracks.items()))\n",
    "                \n",
    "                #only save data if track length fulfills length criterion\n",
    "                if len(dic_tracks) >= track_length:\n",
    "                    \n",
    "                    # as before, test for entry vs no entry\n",
    "                    if 30 in dic_tracks:\n",
    "                        sorting_lines(f_out_ne, dic_tracks, replicate_id, cell_tracker, bleach_correction)\n",
    "                        transposed_lines(f_out_ne_t, dic_tracks, replicate_id, cell_tracker, bleach_correction)\n",
    "                        print(replicate_id, cell_tracker, tr_id_global, 'no entry', sep='\\t', file=f_out_entry)\n",
    "\n",
    "                    elif 30 not in dic_tracks:\n",
    "                        sorting_lines(f_out_e, dic_tracks, replicate_id, cell_tracker, bleach_correction)\n",
    "                        transposed_lines(f_out_e_t, dic_tracks, replicate_id, cell_tracker, bleach_correction)\n",
    "                        print(replicate_id, cell_tracker, tr_id_global, 'entry', sep='\\t', file=f_out_entry)\n",
    "\n",
    "                    else:\n",
    "                        print('neither longer nor shorter than 30 tracks?')\n",
    "                \n",
    "                #shouldn't be necessary anymore\n",
    "                dic_tracks = {}\n",
    "                dic_tracks[tr_frame_global] = []\n",
    "                dic_tracks[tr_frame_global].append(line)\n",
    "                id_identifier = tr_id_global\n",
    "            \n",
    "            #trouble shooting\n",
    "            else:\n",
    "                print('problems reaching last line?')\n",
    "                        \n",
    "                #empty the dic after it is stored in the file and add the data from this line that is from\n",
    "                #a new track ID\n",
    "                #print ('lasted longer than 10 frames')\n",
    "                \n",
    "            \n",
    "    \n",
    "    \n",
    "def sorting_lines (file_name, dictionary, experiment, counter_cell, bleach):\n",
    "    \"\"\"\n",
    "    This function reads the data from the dictionary that was sorted before according to frame (=key of dic).\n",
    "    Data are then saved in a file.\n",
    "    Input values are the file name (the handle from \"with open\" function) as file_name, the sorted dictionary\n",
    "    (dictionary), the replicate id (experiment), a cell counter (counter_cell), and the value for \n",
    "    bleach correction (bleach)\n",
    "    \"\"\"\n",
    "    \n",
    "    for index, key in enumerate (dictionary):\n",
    "        data = str(dictionary[key]).strip().split(',')\n",
    "        tr_id = int(data [2])\n",
    "        tr_frame = int(data [8])\n",
    "        tr_median_1 = float(data [13])\n",
    "        tr_median_2 = float(data [19])\n",
    "        tr_median_3 = float(data [25])\n",
    "        tr_contrast_1 = float(data [32])\n",
    "        tr_contrast_2 = float(data [34])\n",
    "        tr_contrast_3 = float(data [36])\n",
    "        print(experiment, counter_cell, tr_id, tr_frame, \n",
    "              tr_median_1, (tr_median_1+tr_median_1*((tr_frame-1)*((1-bleach)/29))),\n",
    "              tr_median_2, (tr_median_2+tr_median_2*((tr_frame-1)*((1-bleach)/29))),\n",
    "              tr_median_3, (tr_median_3+tr_median_3*((tr_frame-1)*((1-bleach)/29))),\n",
    "              tr_contrast_1, tr_contrast_2, tr_contrast_3, \n",
    "              sep = '\\t', file = file_name)\n",
    "\n",
    "\n",
    "def transposed_lines (file_name, dictionary, experiment, counter_cell, bleach):\n",
    "    \"\"\"\n",
    "    This function writes a new file that contains all values for a given features (e.g., mean intensity\n",
    "    channel 2) in separate rows per track. Values per track will be sorted according to ascending \n",
    "    frame number from left to right. Intended to help with analysis in excel etc.\n",
    "    \n",
    "    Input values are the file name (the handle from with open function) as file_name, the sorted dictionary\n",
    "    (dictionary), the replicate id (experiment), and the value for bleach correction (bleach).\n",
    "    \"\"\"\n",
    "    \n",
    "    #print replicate nr first but stay in same line after printing\n",
    "    print (experiment, counter_cell, file = file_name, sep='\\t', end='\\t')\n",
    "    \n",
    "    #write track ID into file\n",
    "    for index, key in enumerate (dictionary):\n",
    "        data = str(dictionary[key]).strip().split(',')\n",
    "        tr_id = int(data[2])        \n",
    "    print (tr_id, file = file_name, end='\\t')\n",
    "        \n",
    "    #loop through dic and write respective values into file but always in the same line   \n",
    "    for index, key in enumerate (dictionary):\n",
    "        data = str(dictionary[key]).strip().split(',')\n",
    "        #only median 2 used at the moment, all others only to change quickly\n",
    "        tr_frame = int(data [8])\n",
    "        tr_median_1 = float(data [13])\n",
    "        tr_median_2 = float(data [19])\n",
    "        tr_median_3 = float(data [25])\n",
    "        tr_contrast_1 = float(data [32])\n",
    "        tr_contrast_2 = float(data [34])\n",
    "        tr_contrast_3 = float(data [36])\n",
    "        #correct median 2 value for bleaching and write into file\n",
    "        print ((tr_median_2+tr_median_2*((tr_frame-1)*((1-bleach)/29))), file = file_name, end='\\t')\n",
    "    \n",
    "    #when dic is done, print \\n to start new line for next dic from another track ID\n",
    "    print (file = file_name, end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "681ca3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_overview (file_path_overview, file_path_out_overview):\n",
    "    \"\"\"\n",
    "    Function to write an overview file that contains all data from \"branching\" files for better overview.\n",
    "    \n",
    "    Input is file path of input and output file.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open (file_path_overview, 'r') as f_in, open (file_path_out_overview, 'a') as f_out:\n",
    "        \n",
    "        # Test if output file is empty to write header if still empty\n",
    "        if is_file_empty_3(file_path_out_overview):\n",
    "            \n",
    "            print ('replicate', 'cell', 'ID', 'track duration (min)', \n",
    "                   'track distance (micron)', 'track velocity (micron/min)', \n",
    "                   sep='\\t', file=f_out)\n",
    "            \n",
    "        f_in.readline()\n",
    "        \n",
    "        for line in f_in:\n",
    "            data = line.strip().split('\\t')\n",
    "            track_rep = int(data [0])\n",
    "            track_cell = int(data [1])\n",
    "            track_id = int(data [2])\n",
    "            track_duration = float(data [3])\n",
    "            track_distance = float(data [4])\n",
    "            track_veloc = float(data [5])\n",
    "            \n",
    "            print (track_rep, track_cell, track_id, track_duration, track_distance,\n",
    "                  track_veloc, sep='\\t', file=f_out)\n",
    "            \n",
    "def write_overview_tracks_trans (file_path_overview, file_path_out_overview):\n",
    "    \"\"\"\n",
    "    Function to write an overview file that contains all data from transposed \"tracking\" \n",
    "    files for better overview.\n",
    "    \n",
    "    Input is file path of input and output file.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open (file_path_overview, 'r') as f_in, open (file_path_out_overview, 'a') as f_out:\n",
    "        \n",
    "        if is_file_empty_3(file_path_out_overview):\n",
    "            print ('ID', 'cell', 'track', 'median int 2 ch 2 corrected', \n",
    "                   sep='\\t', file=f_out)\n",
    "            \n",
    "        f_in.readline()\n",
    "        \n",
    "        for line in f_in:\n",
    "            data = line.strip().split('\\t')\n",
    "            for i,indexer in enumerate(data): \n",
    "                print ((data[i]), sep='\\t', file=f_out, end='\\t')\n",
    "                \n",
    "            print(end='\\n', file =f_out)\n",
    "            \n",
    "def write_overview_tracks_trans_firstlast (file_path_overview, file_path_out_overview):\n",
    "    \"\"\"\n",
    "    Function to write an overview file that contains all data from transposed \"tracking\" \n",
    "    files for better overview.\n",
    "    \n",
    "    Input is file path of input and output file.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open (file_path_overview, 'r') as f_in, open (file_path_out_overview, 'a') as f_out:\n",
    "        \n",
    "        if is_file_empty_3(file_path_out_overview):\n",
    "            print ('ID', 'cell', 'track', 'median int 2 ch 2 corrected', \n",
    "                   sep='\\t', file=f_out)\n",
    "            \n",
    "        f_in.readline()\n",
    "        \n",
    "        for line in f_in:\n",
    "            data = line.strip().split('\\t')\n",
    "            \n",
    "            print (data[0],data[1],data[2],data[3],data[4],data[5], \n",
    "                   data[-3], data[-2], data[-1], sep='\\t', file=f_out, end='\\t')\n",
    "                \n",
    "            print(end='\\n', file =f_out)\n",
    "            \n",
    "\n",
    "            \n",
    "def write_overview_tracks (file_path_overview, file_path_out_overview):\n",
    "    \"\"\"\n",
    "    Function to write an overview file that contains all data from \"tracking\" files for better overview.\n",
    "    \n",
    "    Input is file path of input and output file.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open (file_path_overview, 'r') as f_in, open (file_path_out_overview, 'a') as f_out:\n",
    "        \n",
    "        if is_file_empty_3(file_path_out_overview):\n",
    "            print ('replicate', 'median int 2 ch 2 corrected', \n",
    "                   sep='\\t', file=f_out)\n",
    "            \n",
    "        f_in.readline()\n",
    "        \n",
    "        for line in f_in:\n",
    "            data = line.strip().split('\\t')\n",
    "            replicate = int(data[0])\n",
    "            median_2 = float(data[7])\n",
    "            print(replicate, median_2, sep='\\t', file=f_out)\n",
    "\n",
    "          \n",
    "\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fcce552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the new files for every single output file and also creates the overview file entry/no entry for \n",
    "# for every track\n",
    "\n",
    "# Import Module\n",
    "import os\n",
    "\n",
    "# Change the directory to the one given in the first code cell with variable \"file_all\"\n",
    "os.chdir(path_all)  \n",
    "\n",
    "# index for file numbers, used to identify data from individual cells and to write \n",
    "# header at the start of output files; currently not used anymore because the actual cell nr from the file\n",
    "# name are used now and because function \"is file empty\" decides about writing the header\n",
    "index = 1\n",
    "\n",
    "# iterate through all files in path defined by variable \"path_all\"; \n",
    "for file in os.listdir():\n",
    "    \n",
    "    # Check whether file is in csv format or not\n",
    "    if file.endswith(\".csv\"):\n",
    "        \n",
    "        series_tracker = ident_cell(file)\n",
    "        replicate = ident_replicate(file)\n",
    "        \n",
    "        file_path = f\"{path_all}{file}\"\n",
    "        \n",
    "        #decide if input file is a summary file (branch) or with data of single points per track (track)\n",
    "        if 'branch' in file:\n",
    "            create_sum(file_path, file, index, replicate, series_tracker)\n",
    "        elif 'track' in file:\n",
    "            create_detail(file_path, file, index, replicate, bleach_cor, series_tracker)\n",
    "        index += 1\n",
    "            \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61196e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates overview files that contain all information for the averaged data (branch files) and all median 2 int\n",
    "# values after bleach correction in either column format or row format\n",
    "\n",
    "# Import Module\n",
    "import os\n",
    "\n",
    "# Change the directory to the one given in the first code cell with variable \"file_sum\"\n",
    "os.chdir(path_all)\n",
    "\n",
    "file_branch_overview = directory + 'branching_overview_all.tsv'\n",
    "\n",
    "file_track_trans_overview_entry = directory + 'tracks-trans_med2c_overview_entry.tsv'\n",
    "file_track_trans_overview_no_entry = directory + 'tracks-trans_med2c_overview_no-entry.tsv'\n",
    "file_track_trans_overview_entry_firstlast = directory + 'tracks-trans_med2c_overview_entry_firstlast.tsv'\n",
    "file_track_trans_overview_no_entry_firstlast = directory + 'tracks-trans_med2c_overview_no-entry_firstlast.tsv'\n",
    "file_track_overview_entry = directory + 'tracks_med2c_overview_entry.tsv'\n",
    "file_track_overview_no_entry = directory + 'tracks_med2c_overview_no-entry.tsv'\n",
    "\n",
    "for file in os.listdir():\n",
    "    \n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".tsv\"):\n",
    "        \n",
    "        series_tracker = ident_cell(file)\n",
    "        file_path = f\"{path_all}{file}\"\n",
    "        \n",
    "        if 'branch' in file:\n",
    "            write_overview(file_path, file_branch_overview)\n",
    "        elif 'track' in file:\n",
    "            if ('median2' in file) and ('_entry_' in file):\n",
    "                write_overview_tracks_trans(file_path, file_track_trans_overview_entry)\n",
    "                #write_overview_tracks_trans_firstlast(file_path, file_track_trans_overview_entry_firstlast)\n",
    "            elif ('median2' in file) and ('_NOentry_' in file):\n",
    "                write_overview_tracks_trans(file_path, file_track_trans_overview_no_entry)\n",
    "                #write_overview_tracks_trans_firstlast(file_path, file_track_trans_overview_no_entry_firstlast)\n",
    "            elif ('bleach' in file) and ('_entry_' in file):\n",
    "                write_overview_tracks(file_path, file_track_overview_entry)\n",
    "            elif ('bleach' in file) and ('_NOentry_' in file):\n",
    "                write_overview_tracks(file_path, file_track_overview_no_entry)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd44e9b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem 2022113021\n",
      "problem 2022113023\n",
      "problem 2022113024\n",
      "problem 2022113025\n"
     ]
    }
   ],
   "source": [
    "# create new overview files for the averaged data depending if a track was identified before to enter or not\n",
    "\n",
    "\n",
    "#file to decide of track entered or not\n",
    "file_enter = path_all + 'entry_vs_no-entry_file.tsv'\n",
    "\n",
    "#branching file that has to be split into entry vs no entry\n",
    "file_branching = directory + 'branching_overview_all.tsv'\n",
    "\n",
    "file_branch_enter = directory + 'branches_enter.tsv'\n",
    "file_branch_no_enter = directory + 'branches_noenter.tsv'\n",
    "\n",
    "with open (file_branching, 'r') as f_in_branch, open (file_enter, 'r') as f_in_enter, open (file_branch_enter, 'w') as f_out_enter, open (file_branch_no_enter, 'w') as f_out_noenter:\n",
    "    \n",
    "    list_entry = []\n",
    "    list_noentry = []\n",
    "    \n",
    "    f_in_enter.readline()\n",
    "    f_in_branch.readline()\n",
    "    \n",
    "    \n",
    "    print ('replicate', 'cell', 'ID', 'track duration (min)', \n",
    "                   'track distance (micron)', 'track velocity (micron/min)', \n",
    "                   sep='\\t', file=f_out_enter)\n",
    "    print ('replicate', 'cell', 'ID', 'track duration (min)', \n",
    "                   'track distance (micron)', 'track velocity (micron/min)', \n",
    "                   sep='\\t', file= f_out_noenter)\n",
    "    \n",
    "    for line in f_in_enter:\n",
    "        data = line.strip().split('\\t')\n",
    "        identifier = data[0] + data[1] + data[2]\n",
    "        if data[-1] == 'entry':\n",
    "            list_entry.append(identifier)\n",
    "        elif data[-1] == 'no entry':\n",
    "            list_noentry.append(identifier)\n",
    "    \n",
    "    \n",
    "    for line in f_in_branch:\n",
    "        data = line.strip().split('\\t')\n",
    "        identifier = data[0] + data[1] + data[2]\n",
    "        if identifier in list_entry:\n",
    "            for i,indexer in enumerate(data): \n",
    "                print ((data[i]), sep='\\t', file=f_out_enter, end='\\t')\n",
    "                \n",
    "            print(end='\\n', file =f_out_enter)\n",
    "        elif identifier in list_noentry:\n",
    "            for i,indexer in enumerate(data): \n",
    "                print ((data[i]), sep='\\t', file=f_out_noenter, end='\\t')\n",
    "                \n",
    "            print(end='\\n', file =f_out_noenter)\n",
    "        else:\n",
    "            print('problem', identifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbce63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2781b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
